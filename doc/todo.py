# Todo - get paths into a configuration file with different profiles
# Todo - complete the project write up
# Todo - Add an Airflow Dag
# Todo - Add checkpointing logic so that we know where to pick up from the last job
# Todo - Add code deployment scripts for the pyspark code (?)
# Todo - Add IAAS deployment scripts for the AWS infrastructure
# Todo - is there any point in a REST or graphql service?
# Todo - Output to Mongo or Snowflake?
# Todo - Add a unique key to the termeratures data so it can be de-duped
# Todo - Add upsert logic so we do not append duplicate records
# Todo - Add a listing of the files in this repo
# Todo - add a few words on how this was or should be or will be run (e.g. the local run into ./datalake
# Todo - add some sort of meta-data where the user can see how current the data is
# Todo - https://itnext.io/running-spark-jobs-on-amazon-emr-with-apache-airflow-2e16647fea0c
# Todo - https://medium.com/swlh/running-pyspark-applications-on-amazon-emr-e536b7a865ca

# ------ done --------------
# _Todo - Add a unique key to the termeratures data so it can be de-duped
# _Todo - complete the project write up
# _Todo - get paths into a configuration file with different profiles

